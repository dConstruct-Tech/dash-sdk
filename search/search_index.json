{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"dConstruct d.ASH SDK","text":"<p>The dConstruct d.ASH SDK is a cross-platform library for autonomous robot navigation. Use the d.ASH SDK to develop applications for your own Spot from Boston Dynamics or any other robot you wish. This section of the d.ASH SDK documentation provides details about the components of the SDK.</p> <p></p> Component Description d.ASH Server  The d.ASH server acts as the main server responsible for sending control commands to the robot. At the same time, the d.ASH server also broadcasts secured data to any given remote systems. d.ASH Autonomy Engine  The d.ASH Autonomy Engine is the autonomy backend code that runs on the edge of the robot. It empowers the robot with robust multi-terrain (indoor and outdoor) autonomous navigation capabilities. It handles secure communication between automony engine and the remote autonomy controller. d.ASH Autonomy Controller  The d.ASH autonomy controller is the GUI (graphical user interface) for the d.ASH autonomy engine. It enable users to monitor and have full remote control of autonomy by allowing users to plot waypoints and activate autonomy on a fleet of robots. Stream real-time data via a secure connection between robots and the controller using 4G or 5G. <p>If you decide to use your own custom GUI in place of the d.ASH autonomy controller, or you do not want to run autonomy, you will still need to implement the d.ASH server and the d.ASH autonomy engine to operate your robot. </p>"},{"location":"dash-auto/autonomy-client/","title":"d.ASH Autonomy Controller","text":"<p>As mentioned previously, the d.ASH autonomy controller is a GUI (graphical user interface) for the d.ASH SDK. It allows users to plot waypoints for autonomous navigation on maps, tracking and monitoring path planning. This section of the d.ASH SDK documentation provides details about setting up the d.ASH autonomy controller, including information on its respective components.</p> d.ASH Autonomy Controller Description  Pair d.C Pilot with d.ASH Autonomy Controller for remote autonomous operations. This app allows you to control multiple robots and author + run autonomous waypoint missions configured with different payloads."},{"location":"dash-auto/autonomy-client/#camera-controls","title":"Camera Controls","text":"<p>You can pan/zoom/fly around the 3D Map Visualization World via the following controls:</p> <ul> <li>Tilt/Camera LookAt: Hold down the Left Mouse Button and drag</li> <li>Zoom: Use the mouse scroll wheel to zoom in/zoom out</li> <li>Move Around: Use the WASD keys to pan/move around the world</li> </ul>"},{"location":"dash-auto/autonomy-client/#general-workflow","title":"General Workflow","text":"<p>The general workflow to operate and run an autonomous waypoint mission is as follows:</p> <ol> <li>Login to the Autonomy Controller</li> <li>Load the point cloud map of your deployment location generated with d.ASH Pack into the app</li> <li>Create a new mission Route and plot the waypoints</li> <li>Connect to your desired robot in your robot fleet</li> <li>Set the robot's initial starting pose on the 3D map</li> <li>Start the robot with the route for your new mission</li> </ol>"},{"location":"dash-auto/autonomy-client/#10-load-point-cloud-map","title":"1.0 Load Point Cloud Map","text":"<p>Click on the Point Cloud icon to open up the point cloud loading window. Then click on Load Point Cloud. A file dialog will pop up to allow you to load your desired point cloud from disk. Make sure the same point cloud map is also configured on the robot(s).</p>"},{"location":"dash-auto/autonomy-client/#11-robot-fleet-manager","title":"1.1 Robot Fleet Manager","text":"<p>The lower expandable panel opens up the Robot Fleet Manager. This displays the list of robots currently registered and also alive under your user account. You connect your desired robot by locating your robot in the icon list and then clicking on the connect icon. When the robot is connected, the following display appears: </p> <p>You are now able to inspect various properties of the robot ( battery status, LIDAR color etc. ) as well as start launching autonomous waypoint missions for this connected robot.</p>"},{"location":"dash-auto/autonomy-client/#12-general-settings","title":"1.2 General Settings","text":"<p>The General Settings expandable panel from the left side of the screen. In here you can configure various options to streamline robot operations.</p> <ul> <li>General: This allows the customization of route/waypoint + map display settings</li> <li>Robot Viz: Visualization settings pertaining to the robots operated in the app</li> <li>Map Viz: Visualization settings for the 3D map display. In particular, use the Floor and Ceiling sliders to adjust how much of the 3D map height-wise </li> </ul>"},{"location":"dash-auto/autonomy-client/#13-routes-waypoints","title":"1.3 Routes + Waypoints","text":"<p>This subpanel can be accessed by expanding the panel on the right side of the screen. It allows you to manage the various mission routes available for your robots. To create a new route, you do the following:</p> <ol> <li>Enter a new Route Name in the textbox provided</li> <li>Click the + button to add a new route</li> <li>Select the newly route by clicking on it</li> <li>Now go into the main 3D Map display to start plotting your waypoints for the route.</li> </ol> <p>You can Append/Splice/Modify a route via the subpanel below: </p> <p>With the desired waypoint plotting option selected, you can go ahead and plot the waypoint directly with the mouse:</p> <p></p> <p>Tap the ESC key to exit the waypoint plotting operation. If you need to delete a waypoint, you can select it via a mouse click on the 3D Map, then press the Delete key.</p>"},{"location":"dash-auto/autonomy-client/#14-settings-the-initial-robot-pose","title":"1.4 Settings the Initial Robot Pose","text":"<p>When you activate a new robot into the app, the first required step is to tell the system where the robot is and where it is facing in relation to the 3D map. This step is defined as setting the Initial Pose of the robot. You set the Initial Pose via the following steps:</p> <ol> <li> <p>Click on the Initial Pose arrow button on the lower expandable panel to start the Initial Pose operation: </p> </li> <li> <p>Go to the 3D map, hold down your mouse on the part of the map where the robot is. Drag a direction and release to set the pose of the robot </p> </li> <li> <p>If the pose was set incorrectly, you can repeat steps 1 and 2 until the LIDAR scans of the robot matches that of the 3D map. This is when you know the Initial Pose is setup correctly.</p> </li> </ol> <p>Take Note: It is very important the Initial Pose of the robot is setup correctly before starting any Waypoint Autonomy Mission! Incorrect initial poses will result in the robot unable to localize correctly leading to mission failure.</p>"},{"location":"dash-auto/autonomy-client/#15-mission-routesmanagement","title":"1.5 Mission Routes/Management","text":"<p>This expandable panel located at the top part of the screen allows you to start new waypoint missions as well as manage existing missions. To start a new mission:</p> <ol> <li>Enter a new mission name in the textbox, then press the + button to add a new mission</li> <li>Select the new mission and press the Run button to start it</li> <li>You should now see your robot move in the 3D map viewer along the mission route</li> </ol>"},{"location":"dash-auto/autonomy-client/#16-robot-camera-streaming","title":"1.6 Robot Camera Streaming","text":"<p>You can activate any available cameras on the robot via the Camera Stream button circled red above. When pressed, a new Camera Stream window pops up:</p> <p></p> <p>Select the robot which you will want to stream the camera from via the top drop down box. Then use the lower drop down box to select which of the available robot cameras to stream from.</p>"},{"location":"dash-auto/autonomy-client/#17-robot-remote-pilotingteleops","title":"1.7 Robot Remote Piloting/TeleOps","text":"<p>The app also allows basic remote manual piloting of your robots. Just like the Pilot Client, plug in a regular joystick into your system to begin manually piloting your robots. You will have to activate the robot for manual control via the manual control button located in the lower expandable panel before piloting is enabled.</p>"},{"location":"dash-pack/dash-pack/","title":"d.ASH Pack","text":"<p>|  |</p> <p>d.ASH Pack is a mobile sensor system that allows users to record 3D point cloud data for various applications, including robot autonomous navigation and Digital Twin generation. d.ASH Pack can either be worn on the back of the user (just like a backpack) or mounted on a robot. It ships together with d.ASH Xplorer application, which is used to generate a 3D point cloud. The entire workflow is fully integrated with d.ASH Fleet Management system.</p> <p>|  |</p> <p>You can view a short video overview of how d.ASH Pack integrates into your Autonomous Robot Deployment workflow here</p>"},{"location":"dash-pack/dash-pack/#requirements","title":"Requirements","text":"<ul> <li>A Windows PC with d.ASH Xplorer</li> <li>d.ASH Pack device</li> </ul> <p>*Users can plug in a 4G USB dongle into the side USB port of d.ASH Pack to remotely control it</p>"},{"location":"dash-pack/dash-pack/#11-quick-start","title":"1.1 Quick Start","text":"<p>A 3D point cloud is created in two steps: Data Collection and Data Processing.</p> <p>Data collection requires the d.ASH Pack device. There are 3 different ways to activate the data collection process, as listed below. If you do not have access to an internet connection, please make sure to use Option 1 which allows you to activate the data collection via your phone.</p> <p>Data processing requires the d.ASH Xplorer application and an internet connection. The d.ASH Xplorer software is used to process the recorded data, create and edit 3D point clouds, and upload data to the Fleet Management cloud.</p> <p>Please ensure that others do not stand near the LiDAR while recording as this could cause undesirable results. Also take note that the d.ASH Pack should not undergo large (1 story tall), sudden changes in elevation while recording.</p> <p>Recording cannot be paused for later continuation. If you would like to pause momentarily, please restart the recording. Ensure some overlap in area between the two separate recordings. Do note that the results may not be as accurate as one full walkthrough.</p> <p>Loop Closure is the act of walking and intersecting paths which you had walked before, an example of this would be walking in the shape of a figure 8, where you meet in the middle and the paths walked intersect one another. Loop Closure is important as it allows for more accuracy during point cloud generation. While recording, try to ensure that there is as much Loop Closure as possible.</p>"},{"location":"dash-pack/dash-pack/#12-connecting-to-dash-pack","title":"1.2 Connecting to d.ASH Pack","text":"<p>The d.ASH Pack Wi-Fi network SSID and Password will be provided to you on the access panel of the d.ASH Pack itself.</p> <p>Use this to connect to your d.ASH Pack and access the user interface by keying this IP Address (https://192.168.10.1/) into a web browser from any device of your choice. Users can check the status of their d.ASH Pack and start/stop recordings here.</p> <p>Mobile interface: |  |</p> <p>Status LEDs The status LEDs are able to show different colours with each colour representing a different status. These LEDs are located on the side of the d.ASH Pack itself. Green LEDs means that the d.ASH Pack has been booted up fully and is ready for recording. Red LEDs means that something has gone wrong with d.ASH Pack. Use the status reflected in your d.ASH Pack's website to help trouble shoot the error. Slow, Flashing Yellow LEDs mean that the d.ASH Pack is currently recording.</p>"},{"location":"dash-pack/dash-pack/#13-data-collection","title":"1.3 Data Collection","text":"<p>Option 1 - Connect d.ASH Pack via Wi-Fi Network (No internet connection required)</p> <ol> <li>Power up d.ASH Pack</li> <li>Connect to d.ASH Pack's Wi-Fi network from your phone or other electronic devices.</li> <li>Open your web browser and key in \"https://192.168.10.1/\".</li> <li>Name your d.ASH Pack recording file and select if you would like imaging (colour) in your scans.</li> <li>Put on the d.ASH Pack and stand stationary for a short moment (e.g. 1s). Then, press start recording.</li> <li>Walk around to cover the area that you would like to record. For better quality, please make sure to walk in loops back to previously visited areas.</li> <li>Once you are done, press stop recording.</li> </ol> <p>Option 2 - Use d.ASH Xplorer</p> <ol> <li>Power up d.ASH Pack</li> <li>Load your d.ASH Xplorer application on your PC</li> <li>Login with your d.ASH credentials.</li> <li>Click on the \"d.ASH Pack Manager\" tab at the top.</li> <li>Select your d.ASH Pack.</li> <li>Click on \"d.ASH Pack Recording Control\"</li> <li>Name your d.ASH Pack recording file and select if you would like imaging (colour) in your scans.</li> <li>Press start</li> <li>Walk around to cover the area that you would like to record. For better quality, please make sure to walk in loops back to previously visited areas.</li> <li>Once you are done, press stop recording.</li> </ol>"},{"location":"dash-pack/dash-pack/#14-data-processing","title":"1.4 Data Processing","text":"<p>For a more in-depth guide, please head to the d.ASH Xplorer documentation page.</p> <ol> <li>Load up the d.ASH Xplorer Application</li> <li>Login and click \"d.ASH Pack Manager\" tab at the top.</li> <li>Your d.ASH Pack device should appear in the list. Select it by clicking on it.</li> <li>Select the recording files that you wish to download.</li> <li>If you connect an Ethernet cable between d.ASH Pack and your PC, you will have options to download the recordings via either an Ethernet cable or wirelessly. For fast download speeds, using an Ethernet cable is recommended.</li> <li>Change 3D point cloud generation configurations to suite the environment of the recording. For generation configuration explanations/tips, please refer to Generation Configs section in Xplorer guide.</li> <li>Select the preferred recording file and click \"Generate Point Cloud\" to start the point cloud generation.</li> <li> <p>While it is running, you will have the following options:     <ul> <li>Pause: Pause the generation process (Appears if generation is running)</li> <li>Resume: Resume the generation process (Appears if generation is paused)</li> <li>Cancel: Cancel the generation process</li> <li>Checkpoint: Export the current 3D point cloud to Point Cloud Editor. This is used to back up the generation progress. If problems arise in the future, some good results can still be restored.</li> </ul></p> </li> <li> <p>Once completed, click \"Point Cloud Editor\" to edit the generated 3D point cloud. The name of the new point cloud is the same as the recording file's name.</p> </li> <li>Follow the d.ASH Xplorer guide to edit the point cloud accordingly.</li> <li>Once you are satisfied with the 3D point cloud, click \"Upload\" to upload the point cloud to your d.ASH Fleet Management account in the cloud.</li> </ol>"},{"location":"dash-pack/dash-pack/#tips-for-data-collection","title":"Tips for Data Collection","text":"<ol> <li>It is recommended to walk in small loops back to previously visited areas for point cloud autocorrection. You will notice some automatic corrections being done during the generation process on d.ASH Xplorer. These corrections are called loop-closures. Walking in the shape of the figure eight is recommended.</li> </ol> <p>Good example of a well defined loop:</p> <p>Bad example where there is no loop:</p> <ol> <li>Where the loops/paths criss-cross should have recognisable static/stationary features (e.g. buildings)</li> </ol> <p>Good examples of easily recognisable features:</p> <p>|  |</p> <p>|  |</p> <p>|  |</p> <p>Bad examples of features (AKA: Dynamic features):</p> <p>|  | </p> <p>|  |</p> <ol> <li>Tighten the d.ASH Pack straps before recording</li> <li>Move at a steady pace</li> <li>Ensure LiDAR is above your head and do not block the sensor when recording (Stand about 5 metres away from person recording)</li> <li>Do not turn quickly in narrow corridors</li> <li>Do not record while in a lift</li> <li>Not recommended to record in narrow stairwells</li> <li>Put on the d.ASH Pack before starting the scan</li> <li>For recording with imaging, please tilt the LiDAR by 30\u00b0. Otherwise a horizontal configuration is recommended</li> <li>If on a vehicle, please dismount and walk through bumpy areas</li> <li>Attempting to loop close long corridors (&gt;20m) is discouraged</li> <li>Avoid large empty areas (e.g. fields) when recording as there are a lack of features</li> </ol>"},{"location":"dash-pack/dash-xplorer/","title":"d.ASH Xplorer","text":"<p>|  |</p> <p>d.ASH Xplorer is the 3D point cloud management application, allowing users to create, edit and export 3D point cloud for various purposes such as for Autonomous Navigation and Digital Twin Applications. d.ASH Xplorer is designed to work with d.ASH Pack and equipped with state-of-the-art SLAM technology for 3D point cloud generation. Users can edit the 3D point cloud by rotating, translating, downsampling, and cleaning up point clouds. 2D maps can also be generated from the 3D point clouds using the built-in grid map generator. The entire point cloud generation workflow is fully integrated with d.ASH Fleet Management to significantly shorten and streamline the preparation process for autonomous navigation.</p> <p>Two versions of d.ASH Xplorer are available. d.ASH Xplorer and d.ASH Xplorer Pro.</p> <p>d.ASH Xplorer Pro is equipped with other add-on features such as the Scan Manager which provides scanning support and automatic stitching of dense 3D point cloud scans. AutoMerge utilises sensor fusion to perform automatic scan alignment and scale-consistent stitching with little human intervention. The AutoMerge system currently supports the Leica BLK360. Other features include uploading of Navigation Maps to the cloud and exporting maps to Revit.</p> <p>Since d.ASH Xplorer is fully integrated with d.ASH Fleet Management system, an internet connection is required. Should you require d.ASH Xplorer without an internet connection, please contact us for more details.</p>"},{"location":"dash-pack/dash-xplorer/#minimum-system-requirements","title":"Minimum System Requirements","text":"<ol> <li>PC with a CPU equivalent to or greater than an Intel i5 4th Gen or AMD R5 2000 series</li> <li>Nvidia GTX 960</li> <li>16GB of RAM</li> <li>Internet Connection</li> <li>Windows 10/11</li> </ol> <p>We recommend using an Nvidia discrete GPU greater than or equivalent to an RTX 3060. Some features such as \"HD View\" are disabled on other GPUs.</p> <p>d.ASH Xplorer is built for Windows 10/11. Therefore, please ensure you are running a Discrete Nvidia GPU in High-Performance mode. Otherwise, some functionalities would be unsupported. You can enable this by going into Windows GPU Settings, and adding d.ASH Xplorer as an app and setting the \"Graphics preference\" to \"High performance\".</p>"},{"location":"dash-pack/dash-xplorer/#tutorial","title":"Tutorial","text":"<p>|  |</p> <p>You can watch the video tutorial to get a quick overview of how to run d.ASH Xplorer above here.</p> <p>Control Scheme</p> <p>d.ASH Xplorer's control scheme is as follows:</p> <ul> <li>WASD: Navigate around the point cloud</li> <li>LMB/MB1: Drag mouse to pan around the point cloud</li> <li>RMB/MB2: Drag mouse to zoom in and out</li> <li>MMB/MB3: Drag mouse to navigate around the point cloud</li> <li>Space: Move upwards(positive) in the Y-axis</li> <li>L Ctrl: Move downwards(negative) in the Y-axis</li> <li>F: Returns view to origin</li> </ul>"},{"location":"dash-pack/dash-xplorer/#credit-system","title":"Credit system","text":"<p>This page is still under construction, check back soon!</p>"},{"location":"dash-pack/dash-xplorer/#21-quick-start","title":"2.1 Quick Start","text":"<p>For a more detailed look at the different features d.ASH Xplorer has to offer, please refer below. This is just a brief overview on how d.ASH Xplorer should be used.</p> <ol> <li>Download the .dpack recording from your d.ASH Pack</li> <li>Click on the Generate Point Cloud button</li> <li>Once the map is generated, configure the Post-processing Settings by changing the Post Processing Options dropdown to Nav Map</li> <li>Then click Post Process</li> <li>Navigate to the Point cloud editor</li> <li>Click Upload Nav Map [d.ASH Xplorer Pro only]</li> </ol> <p>The generated recording can now be used for robot autonomous navigation with d.ASH Fleet Management system.</p>"},{"location":"dash-pack/dash-xplorer/#22-modes","title":"2.2 Modes","text":"<p>d.ASH Xplorer has 2 main modes for various tasks:</p> <ol> <li>Point Cloud Editor: Perform 3D point cloud edits, exports and uploads.</li> </ol> <p>|  |</p> <ol> <li>d.ASH Pack Manager: Control d.ASH Pack and generate 3D point clouds and 2D maps.</li> </ol> <p>|  |</p> <p>d.ASH Xplorer Pro also includes:</p> <ol> <li>Scan Manager: Download point cloud data and perform AutoMerge on 3rd party 3D scanners.</li> </ol> <p>|  |</p> <p>These 3 modes form 3 different tabs at the top of d.ASH Xplorer.</p>"},{"location":"dash-pack/dash-xplorer/#23-point-cloud-editor","title":"2.3 Point Cloud Editor","text":"<p>The point cloud editor is used to manage different point clouds that users have generated. Users can rotate, translate, downsample and perform other 3D point cloud editing features.</p> <p>You can use this mode to visualize the 3D point cloud by using the Load button. File extensions \".pcd\", \".obj\", \".las\" and \".e57\" are currently supported. We also have our own proprietary file extension \".dcloud\" which can be used to load point clouds. We do not recommend exporting large point clouds (&gt; 1gb in size) in the .pcd format. Please export such files in .las, .e57 or .dcloud instead. .dcloud files cannot be used for meshing</p> <p>|  |</p> <p>After loading, your 3D objects will appear in the list under Point Cloud Collections. You can hide or show a point cloud object by clicking the green eye icon.</p> <p>|  |</p> <p>The Remove button simply removes the 3D object from the list. However, it does not delete the file from the PC.</p> <p>|  |</p> <p>The Rename button renames the selected 3D object name.</p> <p>|  |</p> <p>The Export Nav Map button exports not just the selected 3D object, but the 3D object's 2D Grid Map and Configuration files for navigation, to a destination of your choice. When clicked, a file dialogue will pop up for you to choose the save folder destination. 3 different files will be saved. They are the 3D point cloud (.pcd), 2D map (.png), and map configuration (.json).</p> <p>|  |</p> <p>The Export button exports the selected 3D object to a destination of your choice. When clicked, a file dialogue will pop up for you to choose the save folder destination.</p> <p>|  |</p> <p>The HD View button allows for high-resolution views of the full point cloud data including moving and rotating the point cloud in real-time at high frame rates while also being in full colour. An Nvidia discrete gpu with CUDA capabilities is required.</p> <p>|  |</p> <p>The Flip Colour button flips the colours of points in the point cloud from RGB to BGR and vice versa, this may help resolve issues with colours not appearing properly or correctly on your point cloud.</p> <p>|  |</p> <p>The following options are locked to d.ASH Xplorer Pro**</p> <p>The Upload Nav Map button uploads the 3D map to the d.ASH Cloud Fleet Management System. If a map with the same name is found in the cloud, a warning will pop up and ask the user to either overwrite the existing file or cancel the uploading operation. Once uploaded, users can access or download the map from the cloud.</p> <p>|  |</p> <p>The Export to Revit button exports the selected 3D object for use with Revit.</p> <p>|  |</p> <p>The Multi Point Cloud Processor allows users to - align different point clouds through geometry. This is useful for stitching scans of different parts of an area together. - merge the point clouds into a single, final point cloud - compare between different point clouds. There are a few different options for comparing point clouds.</p> <p>To process the point clouds, just select the point clouds you want to process and options to Align, Merge and Compare will appear. A checkbox for the Swap Alignment Indices property will appear as well.</p> <p>|  |</p> <p>Swap Alignment Indices means when alignment is done, the target and source points are swapped (i.e. instead of point A trying to match against point B, point B tries to match against point A) [Default: Unchecked]</p> <p>Aligning point clouds Point clouds taken/generated can be aligned and made to match one another in d.ASH Xplorer. This can aid in comparing and merging of the point clouds. In order to align the point clouds, just click the Align button and d.ASH Xplorer will auto align the point clouds for you.</p> <p>|  |</p> <p>Merging point clouds Point clouds taken/generated can be merged into one point cloud in d.ASH Xplorer. Please ensure the point clouds are aligned first, as merging without aligning can lead to very undesirable results. To merge the point clouds, just click the Merge button.</p> <p>|  |</p> <p>Comparing point clouds Point clouds taken/generated can be compared against one another. Please ensure the point clouds are aligned first, as comparing without aligning can lead to very undesirable results. To compare the point clouds, click the Compare button.</p> <p>|  |</p> <p>The following comparison settings are shown after clicking Compare: - Comparison Resolution (m): Points will be grouped into a voxel the size of the value set. This will be used to compare the point clouds voxel by voxel. [Default: 0.8] - Align Before Comparison: Aligns the point clouds before comparison takes place [Default: Unchecked]</p> <p>|  |</p> <p>Point clouds can be compared with three different measurements which are: - Points added: Comparing point cloud 1 and 2, what are the points added between the two point clouds. This is visualised in green points. - Points removed: Comparing point cloud 1 and 2, which points are removed between the two point clouds. This is visualised in red points. - Points unchanged: Comparing point cloud 1 and 2, what are points which are the same/retained between the two point clouds. This is visualised in blue points.</p> <p>You can enable or disable the different views by clicking on the checkboxes.</p> <p>|  |</p>"},{"location":"dash-pack/dash-xplorer/#24-point-cloud-transformation","title":"2.4 Point Cloud Transformation","text":"<p>This feature allows users to edit the point cloud. Users can perform translation and rotation by using XYZ and quaternion values respectively. There are options for users to reset the transformations back to the original state.</p> <p>In addition, a widget in the centre of the screen, which only appears when the tab is open, is designed to facilitate point cloud transformation. Users can click on the widget and see a live transformation of the 3D object. To toggle between translation and rotation modes of the widget, users can choose the right mode under \"Edit Mode\".</p> <p>|  |</p> <p>Users can perform downsampling from the same dropdown menu and have the ability to set their desired voxel grid size. The values are in Metres. Therefore, a voxel size of 0.2 for downsampling would mean that for every 0.2M HxWxB Cube, all points are removed except for one. Reset to Original resets the 3D point cloud to the original number of points.</p> <p>|  |</p>"},{"location":"dash-pack/dash-xplorer/#25-point-cloud-cleaner","title":"2.5 Point Cloud Cleaner","text":"<p>The Point Cloud Cleaner helps to remove outliers and smoothen the point cloud to make it cleaner. An explanation of the available options is located below the image.</p> <p>|  |</p> <p>Point cloud Denoiser</p> <ul> <li>Denoise Quality: Depending on your system, Rough could still take a few minutes to half an hour. This setting controls the number of iterations the algorithm will run. Refined therefore means the denoise will be more accurate as more iterations of the algorithm will be ran.     [Default: Rough]</li> <li>Denoise Aggressiveness: The denoiser tries to both remove points and also move points to make the point cloud less noisy. The higher this is set, more points will be removed.     [Default: Medium]</li> <li>Chunk Size (m): The point cloud is split into chunks of equal size for denoising. This controls the size of each chunk. For users with less ram, it is recommended to keep to the default or lower the Chunk Size to prevent crashes and other instabilities.     [Default: 50.0]</li> <li>Uniform Sampling: Tries to accentuate surfaces perceived on the point cloud, turn this setting off if shapes are being distorted on the point cloud.     [Default: Unchecked]</li> <li>With Smoothing: Shifting the points slightly to make surfaces more pronounced, turn this setting off if shapes are being distorted on the point cloud.     [Default: Unchecked]</li> </ul> <p>The options listed below are for advanced users, please use them at your own discretion.</p> <p>Statistical Outlier Removal:</p> <ul> <li>K-Mean (Number of Neighbours): For each point, its surrounding points will be used to calculate the standard deviation in distance between points. A higher value means more surrounding points will be considered. [Default: 50]</li> <li>Standard Deviation: A measure of the variation between points. A higher deviation means that points further away from each other will be kept. [Default 2.00]</li> </ul> <p>Point Cloud Smoother:</p> <ul> <li>Radius (m): The searching radius per point. The higher the radius, the more time the denoising will take, but the denoising output will be better. [Default 0.50]</li> <li>Epsilon (Smoothness): The larger the value, the smoother the output will look. [Default 0.20]</li> </ul> <p>Point cloud Resampling:</p> <ul> <li>Polynomial Order: The higher the number, the sharper the edge the denoising can preserve. [Default: 3]</li> <li>Search Radius: Look through all points within the search radius and perform polynomial fitting of the order specified above. [Default: 0.05]</li> </ul> <p>|  |</p>"},{"location":"dash-pack/dash-xplorer/#26-3d-point-cloud-cropper","title":"2.6 3D Point Cloud Cropper","text":"<p>This feature helps users clean up point clouds by cropping them down to a desired section/size. An explanation of the available options are below the image</p> <p>|  |</p> <ul> <li>Visualise Cropping Bounds: Draws the cropping bounds on the preview, allowing users to better see the area remaining after cropping.     [Default: Unchecked]</li> <li>Range X,Y,Z: The cropping ranges, adjust these values to adjust the area remaining after cropping.     [Default: -10.000, 10.000]</li> <li>Auto Set Crop Ranges: d.ASH Xplorer will auto set the crop ranges based on the 3D point cloud.     <li>Crop Point Cloud: Crops the point cloud according to the specified ranges set above.</li> <li>Reset Crop: Resets any crop operations, restoring the original point cloud.</li>"},{"location":"dash-pack/dash-xplorer/#27-2d-map-generator","title":"2.7 2D Map Generator","text":"<p>This feature creates a 2D map from a 3D point cloud by projecting a section of the 3D point cloud to an image file. Users can generate the 2D view from either a top view perspective or side view perspective. Users have 3 different configuration options: min height, max height, and pixel resolution (meter/pixel). To see which region is used for compression, users can check Show Height-Bounds to display the minimum and maximum height planes.</p> <p>|  |</p> <p>Once satisfied, click Generate to apply the configurations and view the 2D map.</p> <p>|  |</p> <p>Users can choose to Save 2D Map separately if needed.</p> <p>It is recommended to ensure that the free space is correctly represented because this information will be used for automatic path-planning and visualization on the website. However, if you do not intend to use d.ASH automatic path-planning, getting a clear 2D map for visualization is sufficient.</p>"},{"location":"dash-pack/dash-xplorer/#28-dash-pack-manager","title":"2.8 d.ASH Pack Manager","text":"<p>This mode allows users to start/stop d.ASH Pack recordings, download d.ASH Pack recordings and generate 3D point clouds through the d.ASH Pack Manager window.</p> <p>Users can only start/stop d.ASH Pack recordings and download d.ASH Pack recording files when d.ASH Xplorer detects that there are online d.ASH Packs. Otherwise, \"No online d.ASH Pack found\" will be shown.</p> <p>|  |</p> <p>If there is an online d.ASH Pack, the d.ASH Pack name will pop up on the list of online d.ASH Pack. Click on it to select the d.ASH Pack device.</p>"},{"location":"dash-pack/dash-xplorer/#29-dash-pack-control","title":"2.9 d.ASH Pack Control","text":"<p>This section allow users to start/stop d.ASH Pack recordings.</p> <ol> <li>Log in to d.ASH Xplorer</li> <li>If offline, connect to d.ASH Pack Wi-Fi</li> <li>Navigate to the d.ASH Pack Manager</li> <li>Before starting the recording, ensure d.ASH Pack is powered on and is emitting a Green light from the status LED, this represents that d.ASH Pack is ready to be used for recording.</li> <li>In d.ASH Xplorer, key in the d.ASH Pack recording name and specify if images should be captured. Images provide colour to the final generated 3D point cloud</li> <li>click the Start button.</li> <li>To monitor the recording status, in d.ASH Xplorer, the d.ASH Pack status will reflect Recording. On the d.ASH Pack itself, the status light will flash Yellow.</li> <li>To stop, click the Stop button. Note that the user can also force stop via pressing the power button, however, only do this as a last resort on the off chance that the stop button isn't functioning.</li> </ol>"},{"location":"dash-pack/dash-xplorer/#210-download-dash-pack-recordings","title":"2.10 Download d.ASH Pack Recordings","text":"<p>After clicking on the list of d.ASH Pack, perform the following steps to download the recording:</p> <p>| </p> <ol> <li>Select the desired d.ASH Pack recording file from the recording list.</li> </ol> <p>| </p> <ol> <li>If there is an ethernet connection between the PC running d.ASH Xplorer and d.ASH Pack, d.ASH Xplorer will use ethernet instead of WiFi to download the recording. It is recommended to download via ethernet for faster downloading speed.</li> </ol> <p>| </p> <ol> <li>Click Download to start downloading.</li> <li>Once it is completed, the downloaded file will appear in the Downloaded d.ASH Pack Recordings list ready for 3D point cloud generation.</li> </ol> <p>|  |</p>"},{"location":"dash-pack/dash-xplorer/#211-3d-point-cloud-generation","title":"2.11 3D Point Cloud Generation","text":"<p>After downloading the d.ASH Pack recording, you can then generate the 3D point cloud for that particular recording.</p> <ol> <li>Under Point Cloud Generation Configs, users can choose different settings for the 3D point cloud generation. For details on the configuration, please refer to the next section</li> </ol> <p>|  |</p> <ol> <li>Select d.ASH Pack recording by clicking on the recording name under the Downloaded d.ASH Pack Recordings list.</li> </ol> <p>|  |</p> <ol> <li>Click Generate Point Cloud to start the 3D point cloud generation. You will see the 3D point cloud being generated progressively on the screen. A green line appearing on the screen represents the path taken during the recording process.</li> <li>While the 3D point cloud is being generated, users will have the following options:</li> </ol> <p>|  |     <ul> <li>Pause: Pause the generation process (Appears if generation is running)</li> <li>Resume: Resume the generation process (Appears if generation is paused)</li> <li>Cancel: Cancel the generation process</li> <li>Checkpoint: Export the current 3D point cloud to the Point Cloud Editor. This is used to back up the 3D point cloud in case there are problems later on.</li> </ul> 5. When generation is completed, the completed point cloud will need to undergo post-processing so the generated point cloud can be used for specific purposes such as for autonomous navigation or to include colour data</p> <ol> <li>The point cloud will now be automatically added to the point cloud list under Point Cloud Editor for other purposes such as editing and uploading.</li> </ol>"},{"location":"dash-pack/dash-xplorer/#post-processing","title":"Post Processing","text":"<p>By clicking on the drop-down menu, the following options to add post-processing effects are shown.</p> <ul> <li>Nav Map: autonomous robot navigation map.</li> <li>Colour Nav Map: autonomous robot navigation with colour.</li> <li>Sparse Coloured Map: coloured point cloud with as few points as possible.</li> <li>Dense Coloured Map: coloured point cloud with many points.</li> </ul> <p>There are several post processing settings for coloured point clouds which are:</p> <ul> <li>Denoise Quality refers to the degree of denoising performed on the generated pointcloud. The Rich setting would have more points than the Sharp setting, the Raw setting means no denoising would take place. We recommend using the Rich setting for indoor areas. [Default: Rich]</li> <li>Max Point Cloud Range per Scan (m) is the distance from the origin where points will be considered and added to the final, post-processed point cloud. Please increase the value when in wider areas such as when post processing outdoor areas. We recommend a minimum of 30.0 for outdoor areas. [Default: 10.0]</li> </ul> <p>|  |</p> <p>When everything has been configured to your desired settings, click on the \"Post Process\" button.</p>"},{"location":"dash-pack/dash-xplorer/#point-cloud-generation-configs","title":"Point Cloud Generation Configs","text":"<p>To ensure desirable generation quality, users may have to edit the default settings to suit their requirements. To reset the settings back to default, click the reset button.</p> <p>Users have the following options:</p> <ol> <li>Start/Stop Time(%): Users can perform 3D point cloud generation for a part of the d.ASH Pack recording by specifying the start and stop time in percentage (from 0-100%) [Default: 0-100%]</li> <li>Number of Threads: Users can manually set the number of threads d.ASH Xplorer can use. The higher the number, the faster the point cloud generation will be. A higher number of threads used for d.ASH Xplorer may cause other system processes to become more laggy. [Default: Depends on numbers of cores on client system]</li> <li>Loop-Closure Similarity Score: The lower this value, the more stringent loop closure will be when finding similar points. Increase this value in 0.10 intervals. [Default: 0.30]</li> <li>Save Point Cloud When Done: When checked, the 3D point cloud will be automatically exported to the Point Cloud Editor when the generation has completed. [Default: Unchecked]</li> <li>Auto-Pause: Point Cloud Generation will be automatically paused when a huge change in position is detected. This is useful for backing up the currently generated data in case of generation failure. [Default: Unchecked]</li> </ol> <p>|  |</p> <p>The Dynamic Point Removal checkbox allows for removal of unwanted objects from the final point cloud (e.g. pedestrians walking past, cars, pets etc) [Default: Unchecked]</p> <p>Use GPU for Inference is essentially hardware acceleration for Dynamic Point Removal. We recommend turning this on. An Nvidia GPU with CUDA is required. [Default: Unchecked]</p> <p>Set Dilate Kernel Size The higher/larger this value, the more points surrounding the moving object/object to be removed, will be removed. [Default: Medium]</p> <p>|  |</p> <p>Advanced Generation Configurations</p> <p>The options listed below are for advanced users, please use them at your own discretion.</p> <ol> <li>Loop Closure Search Radius (m): Used for automatic Loop Closure detection. Scan points within this radius are considered for Loop Closure. [Default: 20.0]</li> <li>Loop Closure Minimum Time Difference (s): A certain amount of time must pass before points can be considered for Loop Closure. This is to prevent consecutive poses from being used as Loop Closure candidates. [Default: 10.000]</li> <li>Loop Closure Sub-Map Search Number: When a loop closure is found, this value will control the number of neighbouring LiDAR scans from each candidate pose which are concatenated together for the purpose of performing scan matching. [Default: 25]</li> <li>Scan-matching Sub-map Search Radius (m): During scan to map matching, this value controls the neighbouring distance from the current pose in which scan matching is done. [Default: 20.000]</li> <li>Minimum Keyframe Distance Difference (m): This value controls the minimum distance between each LiDAR scan which is used for mapping. [Default: 0.50]</li> <li>Minimum Keyframe Angle Difference (rad.): This value controls the minimum angular displacement between each LiDAR scan which is used for mapping. [Default: 0.10]</li> <li>Fast Mode: Uses multiple threads to run map generation. This will be faster than using a single thread but may result in a lower quality map, especially in complex areas with little features such as buildings. [Default: unchecked]</li> <li>Scan Context Loop Closure: Advanced Loop Closure algorithm. Checking this will result in more scan posititions being considered for loop closure, thus generating a higher quality map. [Default: unchecked]</li> </ol> <p>What is Manual Loop Closure Detection? In the event that 3D Point Cloud Generation is unable to detect loop closures (a pair of scan points with similar locations) at certain areas of a point cloud, you can perform Manual Loop Closure Detection to stitch these parts of the point cloud together. Manual Loop Closure Detection works by trying to pair every point selected with each other in all permutations possible.</p> <p>During point cloud generation, if Loop Closures are not detected, pause the generation and perform Manual Loop Closure Detection by following the steps below.</p> <p>Example of a map which requires Manual Loop Closure Detection:</p> <p>|  |</p> <p>What the area should be like: </p> <p>|  |</p> <ol> <li>Select a few pairs of points for Manual Loop Closure Detection. Selected points that should be linked together. An example of this is shown below.</li> </ol> <p>|  |</p> <ol> <li>If you selected the wrong pair of points, highlight the wrong pairs and click the Remove Keyframes button</li> </ol> <p>|  |</p> <ol> <li>Select all remaining pairs and click Optimize and wait for the algorithm to match the points together.</li> </ol> <p>|  |</p> <ol> <li>If no loop closures are able to be detected, increase the similarity score, then attempt optimisation again.</li> </ol> <p>|  |</p> <ol> <li>If needed, continue with manual loop closure detection on other parts of the point cloud.</li> </ol> <p>*Final loop close</p> <p>|  |</p> <p>Manual Loop Closure Detection Settings </p> <ul> <li>Add Neighbour Keyframes: Also adds keyframes beside the selected keyframe. The number of keyframes added is determined by the Neighbour Size value. [Default: Checked]</li> <li>Neighbour Size: The number of neighbour keyframes. [Default: 5]</li> <li>Similarity Score: The lower this value, the more stringent loop closure will be when finding similar points. Increase this value in 0.10 intervals. [Default: 0.30]</li> </ul> <p>These values are only for manual loop closure, automatic loop closure configurations are listed above.</p> <p>Tips</p> <ol> <li>If the Manual Loop Closure Detection fails, users may want to lower the Loop Closure Similarity Score value based on the pop-up.</li> <li>Ensure that there are not too many large point clouds in d.ASH Xplorer's point cloud manager. These are loaded into RAM and can cause instability when too many of such point clouds are loaded.</li> <li>If you are confident that selected points are at the same location but the Loop Closure algorithm has failed to find similarities, please adjust the Loop Closure Similarity Score to a higher value. Once some loop closures are detected, you may observe some adjustments in the generated point cloud. You can then lower the score and perform Loop Closure again on different points to progressively correct the generated point cloud.</li> </ol>"},{"location":"dash-pack/dash-xplorer/#scan-manager-plugin","title":"Scan Manager (Plugin)","text":"<p>This plugin allows users to manage 3rd-party 3D scanners. Currently, the Leica BLK360 scanner is supported. This is currently limited to users of d.ASH Xplorer Pro.</p> <p>This plugin is used to perform the following:</p> <ol> <li>Download scanMeta files from the robot</li> <li>Download scan data from the scanner</li> <li>Perform AutoMerge on all scans.</li> </ol> <p>Download ScanMeta Files</p> <p>scanMeta file (<code>*.scanMeta</code>) holds critical information for each scan point. Each 3D scan activated by the d.ASH robotics stack will generate a scanMeta file. The scanMeta data can be used to perform AutoMerge for creating a digital twin (high accuracy/density 3D point cloud model). scanMeta files are grouped by their project names which are set by d.ASH Autonomy Mission.</p> <p>To download the scanMeta files, perform the following:</p> <ol> <li>Connect your PC running d.ASH Xplorer Pro to the Internet and make sure that the robot is online</li> <li>Log in to d.ASH Xplorer Pro, then connect to the server.</li> <li>Click on the robot from the Online Robot List in the Scan Manager tab.</li> <li>Select the desired data folder by clicking the Folder icon. This folder will be used to store downloaded scanMeta files. We recommend choosing an empty folder. Otherwise, scanMeta files from previous/other projects will be overwritten.</li> <li>Click on Download Files to expand the window.</li> </ol> <p>|  |</p> <ol> <li>Click Download to download ScanMeta files for the entire project.</li> </ol> <p>|  |</p> <ol> <li>After downloading, all ScanMeta files will be stored in the folder selected in Step 3.</li> </ol> <p>Download 3D Scan Data from the Scanner</p> <p>This step performs downloading of 3D scan data from the 3D scanner by using the downloaded ScanMeta files.</p> <ol> <li>Connect your PC running d.ASH Xplorer Pro to the 3D scanner.</li> <li>Select the desired data folder by clicking the Folder icon . This folder should have ScanMeta files.</li> <li>Click on Download Files to expand the window.</li> <li>Under \"Download scan data from scanner\", click Download</li> <li>There will be a window popup showing all ScanMeta filenames found in the folder selected in Step 2. If 3D scan files and ScanMeta files with the same name exist, the filename will have \"[Downloaded]\" appended at the back of their names.</li> <li>Use the checkboxes on the left to mark 3D scan data for downloading. Users can use Select All or Unselect All buttons for file selections.</li> </ol> <p>|  |</p> <ol> <li>Click Download to start the 3D scan downloading process.</li> <li>Once completed, Click Close to close the popup.</li> </ol> <p>AutoMerge</p> <p>This step performs AutoMerge on the 3D Scan data. AutoMerge utilizes sensor fusion techniques to automatically stitch and align multiple 3D scans for scale-consistent digital twin reconstruction. AutoMerge supports both colored and non-colored point clouds. To perform AutoMerge, perform the following steps:</p> <ol> <li>Select the desired data folder by clicking Change. This folder should have both ScanMeta files and 3D scan data files.</li> <li>Click on AutoMerge to expand the window.</li> <li>AutoMerge Scan File List displays a list of files in the selected folder for AutoMerge. Only filenames with <code>.scanMeta</code> and <code>.pcd</code> are considered for AutoMerge.</li> <li>Since AutoMerge relies on the orientation of the 3D scanner to perform stitching, users are encouraged to preview some scans first. This is done by selecting a few scans (greater than two) and clicking Preview.</li> <li>After scan previews have loaded, expand Options and change the Scanner Rotation so that the selected scans are roughly aligned. These rotations will rotate the 3D scan about their centres. As the scanner rotation is changed, the 3D scan previews will also be rotated accordingly. Users do not have to perfectly align the 3D scans manually. Just a rough estimate is sufficient.</li> <li>After configuring the scanner rotation, users can start AutoMerge. Click AutoMerge to start the AutoMerge process on the selected files (greater than one). You will see the 3D scans popping up and aligning themselves automatically after some time.</li> <li>When AutoMerge has completed, users have the following options:     <ul> <li>Export: Save the AutoMerge results and individual scans with corrected poses. Users can choose to export as <code>.pcd</code>, <code>.dcloud</code>, <code>.las</code>, or <code>.e57</code> file formats.</li> <li>Edit: Export the AutoMerge results to Map Editor for editing.</li> </ul></li> </ol> <p>Options</p> <p>There are 3 different options available for Scan Manager:</p> <ol> <li>Scanner Rotation: Rotation in degrees of the scanner relative to the robot heading. As this value is changed, the 3D scan preview will also be updated in real-time.</li> <li>Optimize Visualization: Check this to optimize rendering. Check this if you notice a laggy visualization.</li> <li>Auto save AutoMerge results: Automatically save AutoMerge results to the data folder once AutoMerge has completed.</li> </ol>"},{"location":"dc-pilot/pilot-guide/","title":"dC Pilot Client","text":"<p>The dC pilot is a GUI (graphical user interface) for the d.ASH SDK. It encompasses interactive visual components for you to control your robot both manually and autonomously. Operate your robots safely and precisely, from any location with our reliable and high-performance  BVLOS (Beyond Vision Line of Sight) System with high quality video streams and responsive controls. This section of the d.ASH SDK documentation provides details about using the d.C Pilot Client.</p>"},{"location":"dc-pilot/pilot-guide/#11-introduction","title":"1.1 Introduction","text":"<p>|  |</p> <p>The pilot client allows you to operate your robots safely and precisely, from any location via its high-performance Remote TeleOps/BVLOS (Beyond Vision Line of Sight) system. It is also equipped with high quality video streams and responsive controls for seamless naviation. The system can also be used for fleet management, to discover, monitor and control multiple robots anytime, anywhere with real time video streaming and data collection. You can view a quick introduction of dc Pilot here</p> <p>|  |</p> <p>The Vision AutoDrive is another key feature of the d.C Pilot, using machine learning and computer vision to analyze and understand your robot's surroundings. This allows hands-free Level 2 Autonomy for the navigation of complex, unstructured environments using just cameras alone.  A demonstration of what Vision AutoDrive is capable of can be viewed here</p> <p>Some requirements before starting the d.C Pilot are: </p> <ol> <li>Nvidia CUDA GPU enabled PC ( At least 2 GB of GPU Memory )</li> <li>Joystick connected to the PC</li> <li>1 GB local storage space</li> <li>16 GB of CPU Memory</li> <li>Intel i5 CPU or equivalent</li> <li>Windows 10 64-bit OS or higher</li> </ol>"},{"location":"dc-pilot/pilot-guide/#12-robot-login","title":"1.2 Robot Login","text":"<p>When you first load up the pilot client, you will be asked to login via your user account. After authentication, you will be presented with the following screen below:</p> <p>|  | </p> <p>The screen above shows you the robots switched ON and discovered via dashBoard ( the Fleet Management System ). Any robot currently registered under your user account and is alive will be displayed. Select the robot you want and click on Take Control to proceed to control the robot. If you want to restart the robot ( due to any unforseen previously encountered issues, ), click Restart to do so.</p>"},{"location":"dc-pilot/pilot-guide/#13-main-controls","title":"1.3 Main Controls","text":"<p>|  | </p>"},{"location":"dc-pilot/pilot-guide/#robot-power-onoff","title":"Robot Power ON/OFF","text":"<p>This is an optional step depending on the type of robot you are running. For most robots the robot is automatically turned ON when you physically attach a battery ( or press the actual Power ON button/switch ). For robots that need to be turned ON via software ( like the Boston Dynamics Spot ), you should turn it on by pressing the Power ON button before piloting the robot. Similary, you can Power OFF the robot if it supports such functionality.</p> Basic Manual Piloting  To start the robot from rest, apply pressure on the joystick.  To stop the robot from moving, release your hold on the joystick.  To move the robot forwards, push front on the joystick.  To move the robot backwards, pull back on the joystick.  To turn the robot to the left, tilt left on the joystick.  To turn the robot to the right, tilt right on the joystick.  To get the robot to stand or sit ( if the Robot supports it ), click the <code>stand</code> or <code>sit</code> button under the Basic Control panel on the right side of the main screen."},{"location":"dc-pilot/pilot-guide/#robot-reconnectionrestart-connectivity-issues","title":"Robot Reconnection/Restart ( Connectivity issues )","text":"<p>If you encounter situations where you lose connection to the robot ( or encounter unstable video/control streams ), you should consider Restarting/Reconnecting with the robot. There are 2 options available:</p> <p>|  |</p> <ul> <li>Reconnect: This issues a fast reconnection between the pilot client and the robot. Run this if you suspect there was issue with connectivity between the client and the robot.</li> <li>Full Restart: This will flush + restart the robot system, then reconnect the client to the robot when the robot system is ready. Run this if you need to do a full restart with the robot possibly due to issues other than connectivity.</li> </ul>"},{"location":"dc-pilot/pilot-guide/#14-control-panel","title":"1.4 Control Panel","text":"Control Panel  (1)  \u00a0 Unmute the microphone to allow dual-communication between the pilot client and the robot.  (2)  \u00a0 Toggle between audio to broadcast speakers.  (3)  \u00a0 Record videos in mp4 format.  (4)  \u00a0 Upload/download video recordings.  (5)  \u00a0 Configure settings for your preference ie. night mode.  (6)  \u00a0 Broadcast live video streaming using either a RTSP server or an HSL server."},{"location":"dc-pilot/pilot-guide/#15-basic-controls","title":"1.5 Basic Controls","text":"Component Description  (1) \u00a0 Monitor the joystick position with respect to your robot. Control the robot by pushing further on the joystick.  (2) \u00a0 Adjust the cruise control speed using the slider control or if your joystick has a secondary lever, push the lever to activate.  (3)  \u00a0 Activate auto-drive for your robot to switch to Smart AI Assisted Cruise Autonomy.  -  \u00a0 Use the <code>spacebar</code> shortcut key to activate auto-drive.   -  \u00a0 Use the <code>z</code> shortcut key for your robot to take the next few possible left turns.   -  \u00a0 Use the <code>x</code> shortcut key for your robot to return to forward position after turning left or right.   -  \u00a0 Use the <code>c</code> shortcut key for your robot to take the next few possible right turns."},{"location":"dc-pilot/pilot-guide/#16-cameras","title":"1.6 Cameras","text":"Component Description  (1) \u00a0 Select from a list of cameras onboard Spot, which are automatically detected by the pilot client.   (2) \u00a0 Adjust the order of cameras for a wider view scope. Ticking the flipped settings will adjust the camera orientation.  (3) \u00a0 Click on sync to sync up the default camera layout/settings from the robot. You can also manually change the camera order via the Cameras drop-down combo box.  (4)  \u00a0 Activate human tracking for people detection and labelling."},{"location":"dc-pilot/pilot-guide/#17-autodrive","title":"1.7 AutoDrive","text":"<p>|  | </p> <p>AutoDrive is our state of the art ML/Computer Vision Level 2 Autonomy system for robots. It requires a calibrated 3 camera setup in order to properly function. Please make sure you have the proper setup before continuing. You can watch an overview video of what AutoDrive is capable of here</p> <p>|  | </p> <p>The following options/controls are available:</p> <ul> <li>Run Motors: This starts/stops the AutoDrive system</li> <li>Avoid Grass: Checking this ON/OFF will tell the system whether to make the robot avoid/ignore a grassy area during Autonomy</li> <li>Log Data: This will record any required data for sending during operation</li> </ul>"},{"location":"dc-pilot/pilot-guide/#17-leica-blk360-laser-scanner","title":"1.7 Leica BLK360 Laser Scanner","text":"<p>|  | </p> <p>This panel enables you to run scanning with the Leica BLK360 Laser Scanner. Please make sure the scanner is properly mounted/connected before proceeding. Run the following steps to start scanning:</p> <ol> <li>Type in your Job Name in the textbox. This name will be used for the entire set of scans.</li> <li>Select your Scan quality via the Quality combo box.</li> <li>Select whether you want Color ( None, HDR, LDR ) via the Color combo box.</li> <li>Click Start to start the scanning operation.</li> </ol>"},{"location":"dc-pilot/pilot-guide/#18-leica-rtc360-laser-scanner","title":"1.8 Leica RTC360 Laser Scanner","text":"<p>|  | </p> <p>This panel enables you to run scanning with the Leica RTC360 Laser Scanner. Please make sure the scanner is properly mounted/connected before proceeding.  Run the following steps to start scanning:</p> <ol> <li>Type in your Job Name in the textbox. This name will be used for the entire set of scans.</li> <li>Select your Scan quality via the Quality combo box.</li> <li>Check the options Imaging, Double Scan, VIS for your RTC 360. Please consult your scanner user manual for more information on what those options do.</li> <li>Click Start to start the scanning operation.</li> </ol>"},{"location":"dc-pilot/pilot-guide/#19-boston-dynamics-spot-arm","title":"1.9 Boston Dynamics Spot Arm","text":"<p>|  | </p> <p>This panel enables you to operate the Boston Dynamics Spot Arm if your Spot robot has been configured with one. Please take note that your cameras should be set to Spot's Default Black and White Cameras in order for this to function. </p> <ol> <li>Click Operate to start the Arm operation</li> <li>This will pop up a separate window that shows the black and white cameras on Spot.</li> <li>Click on the desired target area to run the arm manipulation operation.</li> <li>Click Run to start the arm operation.</li> </ol>"},{"location":"dc-pilot/pilot-guide/#110-rtsp-streaming","title":"1.10 RTSP Streaming","text":"<p>This button allows you to start a RTSP stream which can be used to broadcast camera footage from the robot to clients. FFMPEG is required for this feature. However, FFMPEG is not packaged with Dash Pilot. If you would like to use RTSP streaming, please download FFMPEG here and move ffmpeg.exe to the Dash Pilot Application's data folder. This is typically located in C:\\Program Files (x86)\\Dash Pilot.</p> <p>|  | </p> <p>There are several settings for customising the RTSP stream. |  |</p> Setting Default Value  RTSP Channel Please enter only alphabetical letters  RTSP Server NIL  RTSP Cam Index NIL"},{"location":"dc-pilot/pilot-sdk/","title":"dC Pilot SDK","text":"<p>The dC Pilot SDK allows you to develop your own custom robot Teleoperations apps easily. You will need:</p> <ul> <li>C++14 compatible compiler ( VS2022 recommended )</li> <li>For AutoDrive: CUDA enabled GPU</li> <li>Windows 10</li> </ul>"},{"location":"dc-pilot/pilot-sdk/#21-discovering-your-robots","title":"2.1 Discovering your robots","text":"<p>The first step you need to perform is to run the discovery service to find the robot you want registered in your fleet.</p> <p>Here are the required includes: <pre><code>#include &lt;iostream&gt;\n#include &lt;string&gt;\n#include &lt;cxxopts.hpp&gt;\n#include &lt;robotDiscovery.h&gt;\n#include &lt;robotPilot.h&gt;\n#include &lt;robotNode.h&gt;\n#include &lt;unordered_map&gt;\n#include &lt;functional&gt;\n#include &lt;algorithm&gt;\n#include &lt;chrono&gt;\n#include &lt;thread&gt;\n</code></pre></p> <p>Next, start the robot discovery service: <pre><code>// Create the robot discovery service\nauto rDiscovery = DashMicroSv::getRobotDiscovery();\n// Login with your credetials\nif (!rDiscovery-&gt;loginCloud(userName, pwd)) {\nstd::cout &lt;&lt; \"ERROR! Invalid Login Credentials.\" &lt;&lt; std::endl;\nreturn;\n}\n// Start the discovery\nrDiscovery-&gt;startDiscovery();\n</code></pre></p> <p>You can now write a simple function like the one below to find the robot you want: <pre><code>std::shared_ptr&lt;DashMicroSv::RobotNode&gt; findRobotWithName(\nDashMicroSv::RobotDiscovery&amp; rDiscoveryIn, const std::string&amp; robotName, bool printList) {\nstd::shared_ptr&lt;DashMicroSv::RobotNode&gt; retBot = nullptr;\nauto aliveBots = rDiscoveryIn.getAliveRobots();\nif (printList) {\nprintRobotList(aliveBots);\n}\n\nfor (const auto&amp; aBot : aliveBots) {\nif (aBot-&gt;getName() == robotName) {\nretBot = aBot;\nbreak;\n}\n}\nreturn retBot;\n}\n</code></pre></p> <p>So go ahead and retrieve your robot: <pre><code>auto myRobotNode = findRobotWithName(*rDiscovery, \"myRobot\", true);\n</code></pre></p> <p>With your robot found, it is now time to construct the Pilot and control it via teleoperation commands.</p>"},{"location":"dc-pilot/pilot-sdk/#22-teleoperations","title":"2.2 Teleoperations","text":"<p>Use the robot you found in the previous section to construct your Pilot:</p> <pre><code>auto myPilot = std::make_shared(*rDiscovery, *myRobotNode);\n\n// Now connect to the robot\nif(!myPilot-&gt;connect()) {\nstd::cout &lt;&lt; \"ERROR! Unable to connect to robot!\" &lt;&lt; std::endl;\nreturn;\n}\n</code></pre>"},{"location":"dc-pilot/pilot-sdk/#game-loop-ticking-moving-robot-retrieving-images","title":"Game Loop Ticking, Moving Robot, Retrieving images","text":"<p>The recommended implementation to operate the robot is via a simple Game Loop. From there you can tick and send velocity commands as well as retrieve images from your robot. Some pseudo code of how it will look like is presented below:</p> <pre><code>    std::vector&lt;cv::Mat&gt; camImgs;\nwhile(true) {\n\n// Retrieve camera images\nmyPilot-&gt;tick(camImgs);\nProcessCamImgs(camImgs);\n\n// Move robot\nfloat x = 0, y = 0, rot = 0;\nRetrieveJoystick(x, y, rot);\nmyPilot-&gt;setFreeMoveVel(x, y, rot);\n\n// In MS\nPause(1.0);\n}\n</code></pre> <p>This concludes the simple tutorial/writeup on how to use the Pilot SDK to teleoperate your own robots.</p>"},{"location":"getting-started/config-connect/","title":"Configuring Sensors","text":""},{"location":"getting-started/config-connect/#21-velodyne-driver","title":"2.1 Velodyne Driver","text":""},{"location":"getting-started/config-connect/#211-setting-up-on-sensor","title":"2.1.1 Setting Up on Sensor","text":"<p>By default, the Velodyne LIDAR sensor IP address is factory set on default value <code>192.168.1.201</code>. The d.ASH SDK will assume the default Velodyne IP address.</p>"},{"location":"getting-started/config-connect/#212-setting-up-on-personal-computer","title":"2.1.2 Setting Up on Personal Computer","text":"<p>You'll need to configure a static IP address for your computer to use an address within the range <code>192.168.1.XXX</code> where <code>XXX</code> may be any integer from 2 to 254, except 201 (which is the LIDAR\u2019s IP). For example, an appropriate static IP address for your compute could be <code>192.168.1.100</code>. </p>"},{"location":"getting-started/config-connect/#213-testing-velodyne-sensors","title":"2.1.3 Testing Velodyne Sensors","text":"<p>Now, let's test your lidar sensors. To test the Velodyne VLP-16 lidar sensor, run the following command: <pre><code>cd dash_sdk/launch\nroslaunch autonomy_velodyne.launch\n</code></pre> Finally, to check if the ROS messages are published correctly, in another terminal, run the following command: <pre><code>rostopic echo /velodyne_points\n</code></pre></p>"},{"location":"getting-started/config-connect/#22-ouster-driver","title":"2.2 Ouster Driver","text":""},{"location":"getting-started/config-connect/#221-setting-up-on-sensor","title":"2.2.1 Setting Up on Sensor","text":"<p>By default, the Ouster LIDAR sensor IP address is factory set on your IPv6/IPv4 link-local address. The addresses lie within the block <code>169.254.0.0</code> to <code>169.254.255.255</code>. To change the static IP address for Ouster, refer to the Ouster Documentation. It is recommended to set up your own static IP address. </p>"},{"location":"getting-started/config-connect/#222-setting-up-on-personal-computer","title":"2.2.2 Setting Up on Personal Computer","text":"<p>You'll need to configure a static IP address for your computer to use an address within the range <code>192.0.2.XXX</code> where <code>XXX</code> may be any integer from 2 to 254. For example, an appropriate static IP address for your compute could be <code>192.0.2.123</code>. </p>"},{"location":"getting-started/config-connect/#213-testing-ouster-sensors","title":"2.1.3 Testing Ouster Sensors","text":"<p>To test the Ouster OS1-32 lidar sensor, run the following command: <pre><code>cd dash_sdk/launch\nroslaunch autonomy_ouster.launch\n</code></pre> Finally, to check if the ROS messages are published correctly, in another terminal, run the following command: <pre><code>rostopic echo /velodyne_points\n</code></pre></p>"},{"location":"getting-started/config-spot/","title":"1.0 Configuring Spot","text":""},{"location":"getting-started/config-spot/#configuring-spot","title":"Configuring Spot","text":"<p>This section of the d.ASH SDK documentation provides details about setting up Spot with the d.ASH SDK. To configure Spot, you will need to set up on the robot itself and on your personal computer. For further enquiries of setting up Spot, follow Boston Dynamics Documentation.</p>"},{"location":"getting-started/config-spot/#11-setting-up-on-spot","title":"1.1 Setting Up On Spot","text":"<p>By default, user and admin credentials are printed on the label of the robot's battery compartment. Otherwise, you can create your own account through the admin console by creating a user with admin privileges. Check out Boston Dynamics Documentation for more information on how to do so.</p> <p>Remember your credentials!</p> <p>Remember your Spot credentials as you will need those same credentials to set up the next section on your personal computer.</p>"},{"location":"getting-started/config-spot/#12-setting-up-on-pc","title":"1.2 Setting Up On PC","text":"<p>By default, the Spot robot IP address is <code>10.0.0.3</code>. If you have more than one Spot robot, refer to the Boston Dynamics Documentation to use the admin console to change the default IP of additional robots to avoid address conflicts.</p> <p>You'll need to configure a static IP address for your computer to use an address within the range <code>10.0.0.X</code> where <code>X</code> may be any integer from 2 to 254, except 3 (which is the Spot's IP). For example, an appropriate static IP address for your compute could be <code>10.0.0.100</code>. You will then be asked to enter a valid admin or operator username and password. These credentials will match the credentials on Spot's battery compartment.</p>"},{"location":"getting-started/dash-eng/","title":"Interfacing d.ASH Autonomy Engine with ROS","text":"<p>This section of the d.ASH SDK documentation provides details about using ROS with the d.ASH autonomy engine. Described below are ROS topics, along with their type and functionality.</p>"},{"location":"getting-started/dash-eng/#41-publications","title":"4.1 Publications","text":"Topic Type Function <code>/active_path</code> <code>nav_mgs/Path</code> Returns current path being executed. <code>/image</code> <code>sensor_msgs/Image</code> Returns sensor image. <code>/initial_pose</code> <code>geometry_msgs/PoseWithCovarianceStamped</code> Returns initial pose estimate for localization. <code>/localization_status</code> <code>std_msgs/String</code> Returns status of localization certainty. <code>/mcl_pose_marker</code> <code>visualization_msgs/Marker</code> Returns current localization position. <code>/nearest_wpts</code> <code>visualization_msgs/Marker</code> Returns nearest waypoints for the robot to follow. <code>/odom</code> <code>nav_msgs/Odometry</code> Returns odometry reading. <code>/original_path</code> <code>nav_msgs/Path</code> Returns original path before processing. <code>/particle_array</code> <code>geometry_msgs/PoseArray</code> Returns localization particle certainty. <code>/tracking_wpt</code> <code>std_msgs/Float32MultiArray</code> Returns nearest waypoints for the robot to follow."},{"location":"getting-started/dash-eng/#42-subscriptions","title":"4.2 Subscriptions","text":"Topic Type Function <code>/cmd_vel</code> <code>geometry_msgs/Twist</code> Accepts manual command velocity. <code>/imu</code> <code>sensor_msgs/Imu</code> Accepts imu sensor data. <code>/initial_pose</code> <code>geometry_msgs/PoseWithCovarianceStamped</code> Accepts initial pose estimate for localization. <code>/joy</code> <code>sensor_msgs/Joy</code> Accepts joystick message. <code>/move_base_simple/goal</code> <code>geometry_msgs/PoseStamped</code> Accepts final goal from RVIZ. <code>/odom</code> <code>nav_msgs/Odometry</code> Accepts odometry reading. <code>/lidar_points</code> <code>sensor_msgs/PointCloud2</code> Accepts lidar scan."},{"location":"getting-started/map-loading/","title":"Map Loading","text":"<p>This section of the d.ASH SDK documentation provides details about file organisation of autonomy maps for the d.ASH SDK. </p> <p>To load a new map, upload the autonomy map files in the folder <code>maps</code> found in <code>/dash_sdk/.data/maps</code>. Please ensure that the following files are in the folder:</p> <pre><code>dash-sdk/\n\u2514\u2500 .data/\n    \u2514\u2500 maps\n        \u2514\u2500 &lt;MAP_NAME&gt;.png       # 2D Autonomy Map\n        \u2514\u2500 &lt;MAP_NAME&gt;.pcd       # 3D Autonomy Map\n        \u2514\u2500 &lt;MAP_NAME&gt;.json      # Global Planner Configuration\n</code></pre> <p>To activate the new map, ensure the map name in <code>auto_config.json</code> file matches <code>&lt;MAP_NAME&gt;</code>. For example: </p> <pre><code>\"map_name\": \"outdoor_map\",\n</code></pre>"},{"location":"sdk-config/auto-config/","title":"Auto Configuration","text":"<p>This section of the d.ASH SDK documentation provides details about the configuration file for the robot - <code>auto_config.json</code> - found in the folder <code>\\dash-sdk\\configs</code>. Information in this section includes variable and definitions to configure autonomy.</p>"},{"location":"sdk-config/auto-config/#41-config-file","title":"4.1 Config File","text":"<pre><code>{ \n    \"py_address\"                            : \"0.0.0.0:50051\",\n    \"ue_address\"                            : \"0.0.0.0:50052\",\n    \"ssl\"                                   : true,\n    \"motion_planner\"                        : true,\n    \"localization\"                          : true,\n    \"sim_mode\"                                : false,\n    \"send_data_gui\"                         : true,\n    \"camera\"                                : \"RealsenseCam\",\n    \"retrieveImg\"                           : false,\n    \"map_name\"                              : \"office_lvl4\",\n    \"pc_topic\"                              : \"velodyne_points\",\n    \"odom_topic\"                            : \"odom\",\n\n    \"controller\":{\n        \"linear_window\"                     : 0.5,\n        \"linear_min_v\"                      : 0.0,\n        \"linear_max_v\"                      : 0.8,\n        \"angular_max_w\"                     : 3.142,\n        \"linear_max_a\"                      : 1.0,\n        \"angular_max_a\"                     : 5.0,\n        \"robot_width\"                       : 0.4,\n        \"robot_length\"                      : 1.0,\n        \"obstacle_cost_gains\"               : 3.0,\n        \"speed_cost_gains\"                  : 1.0,\n        \"goal_cost_gains\"                   : 4.0,\n        \"angular_speed_cost_scaling_factor\" : 0.1,\n        \"linear_num_window_steps\"           : 50,\n        \"angular_num_window_steps\"          : 30,\n        \"prediction_window\"                 : 5.0,\n        \"costmap_size\"                      : 20.0,\n        \"costmap_scale\"                     : 0.1,\n        \"max_pc_height\"                     : 0.2,\n        \"min_pc_height\"                     : -0.5,\n        \"x_filter\"                          :[-0.2, 0.2],\n        \"y_filter\"                          :[-0.1, 0.1],\n        \"costmap_obs_inflation\"             : 1.0,\n        \"occ_obs_deadzone\"                  : 0.2,\n        \"dt\"                                : 0.1,\n        \"visualise\"                         : false\n    },\n    \"state_estimator\":{\n        \"initial_x\"                         : -7.7, \n        \"initial_y\"                         : -14.5, \n        \"initial_z\"                         : 1.0, \n        \"initial_w\"                         : -0.177,     \n        \"kImuTopic\"                         : \"imu\", \n        \"kPoseTopic\"                        : \"mcl_pose\",\n        \"ktfUpdate\"                         : 0.02,\n        \"kStatusUpdate\"                     : 1.0,\n        \"kLoggingUpdate\"                    : 15.0,\n        \"kposeDiffmax\"                      : 5.0, \n        \"KUse_imu_ori\"                      : false,\n        \"kBadCovThres\"                      : 2.0,\n        \"kGoodCovThres\"                     : 0.7,\n        \"kCovBadMax\"                        : 10, \n        \"kCovGoodtMax\"                      : 5,\n        \"kFilter_z\"                         : true,\n        \"klimit_min\"                        : -0.3,\n        \"klimit_max\"                        : 5.0 \n    },\n    \"planner\":{\n        \"lookAheadIndex\"                    : 15,\n        \"enable_self_rotate\"                : false,\n        \"self_rotation_speed\"               : 0.5,\n        \"self_rotation_speed_final\"         : 0.3,\n        \"dis_threshold\"                     : 0.5,\n        \"theta_threshold\"                   : 0.2,\n        \"cmd_Smoothing\"                     : true,\n    }\n}\n</code></pre>"},{"location":"sdk-config/auto-config/#42-definitions","title":"4.2 Definitions","text":""},{"location":"sdk-config/auto-config/#421-main","title":"4.2.1 Main","text":"Variable Definition <code>py_address</code> The address of the d.ASH server in the formal <code>&lt;IP&gt;:&lt;PORT&gt;</code>. <code>ue_address</code> The address of the GUI server in the formal <code>&lt;IP&gt;:&lt;PORT&gt;</code>. <code>ssl</code> Enables secure SSL messaging and encryption. <code>motion_planner</code> Enables autonomy motion planning. <code>localization</code> Enables robot localisation, returning users position and orientation in relation to map. <code>sim_mode</code> Enables Spot odometry retrieval. <code>send_data_gui</code> Enables ability to send data to GUI server for visualisation. <code>camera</code> Camera active for the current session to retrieve data ie. RealsenseCam, TestCam. <code>retrieveImg</code> Enables image retrieval. <code>map_name</code> Map name used for autonomy (as mentioned in File Organisation). <code>pc_topic</code> ROS point cloud topic name for subscribing <code>odom_topic</code> ROS odometry topic name for subscribing."},{"location":"sdk-config/auto-config/#422-controller","title":"4.2.2 Controller","text":"<p>For the following parameters, ensure the value is within limits of the robot as per its documentation.</p> Variable Definition <code>linear_window</code> Sets DWA (dynamic window approach) size. <code>linear_min_v</code> Sets minimum linear velocity for autonomy. <code>linear_max_v</code> Sets maximum linear velocity for autonomy. <code>angular_max_w</code> Sets maximum angular velocity for autonomy. <code>linear_max_a</code> Sets maximum linear acceleration for autonomy. <code>angular_max_a</code> Sets maximum angular acceleration for autonomy. <code>robot_width</code> Reflects width of robot. <code>robot_length</code> Reflects the length of robot. <code>obstacle_cost_gains</code> Sets weight for an obstacle course based on the weighted sum of the map. <code>speed_cost_gains</code> Sets weight for speed cost. <code>goal_cost_gains</code> Sets weight for goal cost. <code>angular_speed_cost_scaling_factor</code> Sets weight for angular velocity. Note that a higher value of the variable discourages the robot from turning. <code>linear_num_window_steps</code> Sets number of linear velocity values to consider. Note that a higher value of the variable slows down the computations. <code>angular_num_window_steps</code> Sets number of angular velocity values to consider. Note that a higher value of the variable slows down the computations. <code>prediction_window</code> Sets prediction horizion (in seconds). Note that a higher value of the variable slows down the computations. <code>costmap_size</code> Sets local costmap size (in meters). <code>costmap_scale</code> Sets scale to convert map from meter to pixels. <code>max_pc_height</code> Sets maximum point cloud height to be considered as an obstacle. <code>min_pc_height</code> Sets minimum point cloud height to be considered as an obstacle. Note that the point cloud height is measured from the center of the lidar. If the ground is detected, it will be considered as an obstacle. Therefore, set the minimum value to be above the ground. <code>x_filter</code> Sets vector of size 2 consisting the minimum and maximum x-value of point cloud to be removed. Do not remove too much from the point cloud filter as obstacles around the robot might not be considered. <code>y_filter</code> Sets vector of size 2 consisting the minimum and maximum y-value of point cloud to be removed. Do not remove too much from the point cloud filter as obstacles around the robot might not be considered. <code>costmap_obs_inflation</code> Sets inflation radius of obstacles to be considered in planning. Note that a higher value of the variable results in more conservative planning. <code>occ_obs_deadzone</code> Sets minimum distances from obstacles and robots for autonomy. <code>dt</code> Sets timestep. Note that a higher timestamp slows down the computation. <code>visualise</code> Enables visualisation of costmap. This is used only for debugging."},{"location":"sdk-config/auto-config/#423-state-estimator","title":"4.2.3 State Estimator","text":"Variable Definition <code>initial_x</code> Sets initialization of x-axis for localizaition (in meters). <code>initial_y</code> Sets initialization of y-axis for localizaition (in meters). <code>initial_z</code> Sets initialization of z-axis for localizaition (in meters). <code>initial_w</code> Sets initialization of orientation for localizaition. <code>kImuTopic</code> ROS IMU (Inertial Measurement Unit) topic name for subscribing. <code>kPoseTopic</code> Enables localization result. <code>ktfUpdate</code> Sets ROS tf publishing frequency. <code>kStatusUpdate</code> Sets localisation status of publishing frequency. <code>kLoggingUpdate</code> Sets data logging period. <code>kposeDiffmax</code> Sets the maximum distance between two consecutive pose estimation. <code>KUse_imu_ori</code> Enables IMU (Inertial Measurement Unit) or odom orientation for odometry estimation. If this variable is set to true, ensure <code>kImuTopic</code> is available. <code>kBadCovThres</code> Sets localization quality. <code>kGoodCovThres</code> Sets localization quality. <code>kCovBadMax</code> Sets localization quality. <code>kCovGoodtMax</code> Sets localization quality. <code>kFilter_z</code> Enables pass through filter application for localization. <code>klimit_min</code> Sets minimum range of pass through filter. <code>klimit_max</code> Sets maximum range of pass through filter."},{"location":"sdk-config/auto-config/#424-planner","title":"4.2.4 Planner","text":"Variable Definition <code>lookAheadIndexv</code> Sets look-ahead index from the nearest waypoint for path to follow. Note that a lower index slows down the movement of the robot. Similarly, a higher index results in the robot not follow path properly. <code>enable_self_rotate</code> Enables one round of rotation around the robot itself before performing autonomy. This is to ensure that localisation is working before starting autonomy. <code>self_rotation_speed</code> Sets angular velocity of robot to turn around itself before performing autonomy (in radiants/second). <code>self_rotation_speed_final</code> Sets angular velocity of robot to turn around itself after performing autonomy (in radiants/second). This ensures that the final orientation of robot aligns with its goal. <code>dis_threshold</code> Sets maximum euclidean distance from the robot to the final goal for destination to be considered having reached its goal. Note that a smaller threshold discourages the robot from determing if it has reached its goal. <code>theta_threshold</code> Sets maximum orientation distance from the robot to the final goal for destination to be considered having reached its goal. Note that a smaller threshold discourages the robot from determing if it has reached its goal. <code>cmd_Smoothing</code> Enables smoothing control commands."},{"location":"sdk-config/register-bot/","title":"Register Payload Configuration","text":"<p>This section of the d.ASH SDK documentation provides details about the configuration file for payload registration - <code>register_payload_config</code> - found in the folder <code>\\dash-sdk\\configs</code>. Information in this section includes variable and definitions to configure payload registration.</p>"},{"location":"sdk-config/register-bot/#21-config-file","title":"2.1 Config File","text":"<pre><code>{\n    \"robot_name\" : \"&lt;ROBOT_NAME&gt;\",\n    \"robot_username\"  : \"&lt;DC_USERNAME&gt;\"\n}\n</code></pre>"},{"location":"sdk-config/register-bot/#22-definitions","title":"2.2 Definitions","text":"Variable Definition <code>robotName</code> Set the name of your robot - this can be any string. <code>robotUserName</code> Set the name of your username - this has to match your dConstruct cloud admin username."},{"location":"sdk-config/rest-config/","title":"d.ASH Service Configuration","text":"<p>This section of the d.ASH SDK documentation provides details about the configuration file for the rest server - <code>dash_service_config.json</code> - found in the folder <code>\\dash-sdk\\configs</code>. Information in this section includes variable and definitions to configure the d.ASH service.</p>"},{"location":"sdk-config/rest-config/#11-config-file","title":"1.1 Config File","text":"<pre><code>{\n    \"port\" : 3000,\n    \"cert_filename\" : \"./cert.pem\",\n    \"key_filename\" : \"./key.pem\",\n    \"dh_params_filename\" : \"\",\n    \"robot_register_native_cert\" : true,\n    \"active_IP_idx\" : 1, \n    \"preferred_IP\" : \"10.8.0.5\", \n    \"run_cmds\" : {\n        \"py_server\" : {\n            \"cmd_str\" : \"python ./spot_server.py ./configs/dash-service-config.json &lt;!TOKEN!&gt;\",\n            \"cmd_path\" : \"C:/Users/dc/Documents/Projects/dc/dash_code/py_server\"\n        }\n    }\n}\n</code></pre>"},{"location":"sdk-config/rest-config/#12-definitions","title":"1.2 Definitions","text":""},{"location":"sdk-config/rest-config/#121-main","title":"1.2.1 Main","text":"Variable Definition <code>port</code> Fixed port number. <code>cert_filename</code> Fixed certification filename. <code>key_filename</code> Fixed certification key filename. <code>dh_params_filename</code> Fixed parameter filename. <code>robot_register_native_cert</code> If you registed with register_bot_native, the data will be encrypted. Always set this variable to true. <code>active_IP_idx</code> Selects the IP address you will send to the backend cloud service for robot discovery. This is an integer index ( from 0 to N ), based on the IP addresses available on your system. When the rest service starts up, you should see a list. Set the value to the appropriate index you want. This index will map the the IP which the clients will try to connect to. <code>preferred_IP</code> Selects your preferred IP from the list of IPs. Specify the IP as a string in this case."},{"location":"sdk-config/rest-config/#122-dash-server-commands","title":"1.2.2 d.ASH Server Commands","text":"Variable Definition <code>cmd_str</code> Sets command to run d.ASH server. <code>cmd_path</code> Sets command path."},{"location":"sdk-config/robot-config/","title":"Robot Configuration","text":"<p>This section of the d.ASH SDK documentation provides details about the configuration file for the robot - <code>robot_config.json</code> - found in the folder <code>\\dash-sdk\\configs</code>. Information in this section includes variable and definitions used to configure the d.ASH server.</p>"},{"location":"sdk-config/robot-config/#31-config-file","title":"3.1 Config File","text":"<pre><code>{\n\"server_address\" : \"localhost:50051\",\n\"robot_hostname\" : \"192.168.80.3\",\n\"username\" : \"&lt;USERNAME&gt;\",\n\"cam_list\" : [\"RealsenseCam\"],\n\"payloads\" : [],\n\"data_state_log_folder\" : \"G:/Temp/logs\",\n\"ssl\" : true,\n\"fast_server\" : false,\n\"fast_server_hostname\" : \"localhost:7777\",\n\"secure_default_token\" : false,\n\"test_mode\" : true,\n\"with_audio\" : true,\n\"real_sense_config\" : {\n\"test\" : true,\n\"test_filenames\" : [\"../../test_videos/nus_left.mp4\", \"../../test_videos/nus_center.mp4\",\"../../test_videos/nus_right.mp4\"],\n\"flip_options\" : {\n\"0\" : [false, false],\n\"1\" : [true, true],\n\"2\" : [false, false]\n},\n\"base_width\" : 640,\n\"base_height\" : 480,\n\"codec\" : \"video\",\n\"width\" : 320,\n\"height\" : 240,\n\"bitrate\" : 3600000\n}\n}\n</code></pre>"},{"location":"sdk-config/robot-config/#32-definitions","title":"3.2 Definitions","text":""},{"location":"sdk-config/robot-config/#321-main","title":"3.2.1 Main","text":"Variable Definition <code>server_address</code> Sets address of the d.ASH server in <code>&lt;HOSTNAME&gt;:&lt;PORT&gt;</code> format. <code>robot_hostname</code> Sets hostname of the Spot to connect to robot's IP. <code>username</code> Sets username for d.ASH server credentials. <code>cam_list</code> Sets a list of cameras active for the current session. <code>payloads</code> Optional payloads list. <code>data_state_log_folder</code> Sets folder to write out the recorded msgpack data of the robot. <code>ssl</code> Enables secure SSL messaging and encryption. <code>test_mode</code> Enables the d.ASH server to enter into test mode. <code>with_audio</code> Enables audio streaming playback."},{"location":"sdk-config/robot-config/#322-intel-realsense-configuration","title":"3.2.2 Intel RealSense Configuration","text":"Variable Definition <code>test</code> Enables simulation of camera streaming via provided custom mp4 video files specified as a list in <code>test_filenames</code>. <code>test_filenames</code> List of test files. <code>flip_options</code> Specify how each camera flips long the x-axis and y-axis following the format <code>{\"index\" : [x-flip, y-flip]}</code>. <code>baseWidth</code> Sets processing width of the camera stream. Note that a minimum <code>baseWidth</code> of 640 is required. <code>baseHeight</code> Sets the processing height of the camera stream. Note that a  minimum <code>baseHeight</code> of 360 is required. <code>codec</code> Sets jpg/video options, with jpg being regular jpeg encoding and video using VP9 encoding. <code>width</code> Adjusts the final returned/resized width dimensions of the input camera stream. Video will be processed at this base resolution before being resized via <code>width</code> and <code>height</code>. Use these variables to change the actual processed resolution for power/efficiency considerations of the RealSense devices. <code>height</code> Adjusts the final returned/resized height dimensions of the input camera stream. Video will be processed at this base resolution before being resized via <code>width</code> and <code>height</code>. Use these variables to change the actual processed resolution for power/efficiency considerations of the RealSense devices. <code>bitrate</code> This is the quality of the video encoding. Note for HD Streaming, RealSense requires a high bitrate of 3600000."},{"location":"setup/dash/","title":"Setting Up d.ASH","text":"<p>As mentioned previously, the d.ASH consists of three main components - d.ASH server, d.ASH service, and d.ASH autonomy engine. This section of the d.ASH SDK documentation provides details about setting up the d.ASH components including compiling and testing.</p>"},{"location":"setup/dash/#41-installing-dash-dependencies","title":"4.1 Installing d.ASH Dependencies","text":"<p>To install the remaining d.ASH dependencies using pip, the following desktop dependencies must be set up prior:</p> <ul> <li> Intel RealSense SDK 2.0</li> <li> ROS Melodic on Ubuntu 18.04</li> <li> FFmpeg and others</li> </ul> <p>If the above dependencies have been installed, proceed by running the following command to install the rest of the python packages: <pre><code>python3.7 config_libs.py\n</code></pre> Please ensure you are in the <code>\\dash-sdk</code> directory before running. Following the instructions prompted by the terminal to proceed with installation.</p>"},{"location":"setup/dash/#42-setting-up-dash-server","title":"4.2 Setting up d.ASH Server","text":"<p>To set up the d.ASH server, you will need to configure the d.ASH server configuration file - <code>robot_config.json</code> - located in the folder <code>\\dash-sdk\\configs</code>. </p> <pre><code>dash-sdk/\n\u2514\u2500 configs/\n    \u2514\u2500 robot_config.json\n    \u2514\u2500 ...\n</code></pre> <p>Follow the variable definitions for <code>robot_config.json</code> to set up the file correctly for the d.ASH server.</p> <p>Once <code>robot_config.json</code> has been set up, run the d.ASH server by executing the following command on your terminal:</p> <pre><code>python3.7 ./dash_server.py robot_config.json\n</code></pre>"},{"location":"setup/dash/#43-setting-up-dash-service","title":"4.3 Setting up d.ASH Service","text":"<p>To set up the d.ASH service, you will need to configure the d.ASH service configuration file - <code>dash_service_config.json</code> - located in the folder <code>\\dash-sdk\\configs</code>. </p> <pre><code>dash-sdk/\n\u2514\u2500 configs/\n    \u2514\u2500 dash_service_config.json\n    \u2514\u2500 ...\n</code></pre> <p>First, run <code>runrest</code> to see available IP address for your rest server: <pre><code>runrest\n</code></pre> Pick the index of the IP address you like and append it to the <code>activeIPIdx</code> variable in <code>dash_service_config.json</code>: <pre><code>\"activeIPIdx\" : 1, # where '1' is the chosen IP address index\n</code></pre> Then, you will need to set your <code>preferredIP</code> address, that is, the IP address for the computer onboard your robot. This IP address will have precedence over <code>activeIPIdx</code>. Similarly, replace the default IP address with your preferred IP address in <code>dash_service_config.json</code>: <pre><code>\"preferredIP\" : \"10.8.0.5\", # where '10.8.0.5' is the preferred IP address\n</code></pre></p> <p>Ensure that the IP address of the onboard computer is within the same subnet by the remote client. Lastly, you will need to change the <code>&lt;PATH_OF_SDK&gt;</code> of <code>cmdPath</code> in <code>dash_service_config.json</code>:  <pre><code>\"cmdPath\" : \"&lt;PATH&gt;\"\n</code></pre></p> <p>To do this, use <code>pwd</code> to print your current working directory path and replace <code>&lt;PATH_OF_SDK&gt;</code> with the path printed. For example, if your current directory is <code>/home/dash_sdk/py_server</code>:</p> <pre><code>\"cmdPath\" : \"/home/dash_sdk\"\n</code></pre> <p>To test the d.ASH service, you'll need to run the d.ASH server by running the following command: </p> <pre><code>./robot_rest &lt;PATH_TO_SDK&gt;/configs/dash_service_config.json\n</code></pre> <p>Now that your d.ASH service is running, you can use our d.ASH Pilot app. To launch the  d.ASH Pilot app, simply search for it in your Windows search bar. Now, login to the system and connect to your robot to start controlling your robot. For more information on the d.ASH Pilot, refer to the d.ASH Pilot guide.</p>"},{"location":"setup/dash/#44-setting-up-dash-autonomy-engine","title":"4.4 Setting up d.ASH Autonomy Engine","text":"<p>To set up the d.ASH autonomy engine, you will need to configure the d.ASH autonomy configuration file - <code>auto_config.json</code> - located in the folder <code>/dash-sdk/configs/</code>.</p> <pre><code>dash-sdk/\n\u2514\u2500 configs/\n    \u2514\u2500 auto_config.json\n    \u2514\u2500 ...\n</code></pre> <p>Follow the variable definitions for <code>auto_config.json</code> to set up the file correctly for the d.ASH autonomy engine.</p> <p>Once <code>auto_config.json</code> has been set up, test the d.ASH autonomy engine by run the executable below to start autonomy driver. Note to replace <code>&lt;PATH_TO_SDK&gt;</code> with your current working directory containing the d.ASH SDK:</p> <pre><code>./dash_autonomy &lt;PATH_TO_SDK&gt;/config/auto_config.json\n</code></pre> <p>To find your current working directory, use <code>pwd</code>. For example, if your directory is <code>/home/dash-sdk</code>, you would run the following command to test d.ASH autonomy:</p> <pre><code>./dash_autonomy /home/dash-sdk/config/auto_config.json\n</code></pre> <p>You will need to run d.ASH service first before running the d.ASH server and the d.ASH autonomy engine.</p> <p>On a seperate terminal, start a simple <code>roslaunch</code> test by running the following prompt, replacing <code>&lt;PATH_TO_SDK&gt;</code> with your current working directory containing the d.ASH SDK:</p> <pre><code>cd \\launch\nroslaunch &lt;PATH_TO_SDK&gt;\\dash_sdk\\launch\\simple_joy.launch\n</code></pre> <p>Launch files have been prepared for setup - either with the Velodyne VLP-16 lidar sensor or the Ouster OS1-32 lidar sensor. These lauch files can be found in under the <code>\\dash-sdk\\launch</code> folder. You can also create your own sensor launch files for your tests.</p>"},{"location":"setup/desktop-dep/","title":"Installing Dependencies on the Desktop","text":"<p>While most of the d.ASH SDK build is hermetic, some system dependencies on the Desktop are required. This section of the d.ASH SDK documentation provides details for Intel RealSense SDK, Ubuntu, ROS Melodic, and FFmpeg installations.</p>"},{"location":"setup/desktop-dep/#11-ubuntu-installation","title":"1.1 Ubuntu Installation","text":"<p>Ubuntu is a complete Linux operating system, which will serve as the primary platform for ROS. Currently, the d.ASH SDK only supports Linux Ubuntu 18.04 LTS for development and simulation from your workstation. </p> <p>Ubuntu Installation via Bootable USB</p> <p>If you would like to install Ubuntu via a bootable USB, you can do so for both Windows and MacOS.</p> <p>Ubuntu Installation via Virtual Machine</p> <p>If you would like to install Ubuntu via a virtual machine, you can do so using VirtualBox to kickstart your installation process.</p> <p>Once Ubuntu is installed, check that your version of Ubuntu has the release code <code>18.04</code>.  Open the terminal and type the command: <pre><code>lsb_release -a\n</code></pre> This should print the following result: <pre><code>No LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 18.04.5 LTS\nRelease:        18.04\nCodename:       bionic\n</code></pre></p>"},{"location":"setup/desktop-dep/#12-ros-installation","title":"1.2 ROS Installation","text":"<p>ROS (Robot Operating System) is a open-source framework that helps researchers and developers build and reuse code between robotics applications. Currently, the d.ASH SDK only supports ROS Melodic for development and simulation from your workstation. </p> <p>Use Ubuntu to install ROS Melodic onto your computer system. Once ROS has been installed on your Linux system, check that your version of ROS is <code>melodic</code>.  Open the terminal and type the command:</p> <pre><code>rosversion -d\n</code></pre> <p>This should print <code>melodic</code>. Otherwise, ensure that you installed the correct version of ROS - ROS Melodic.</p>"},{"location":"setup/desktop-dep/#13-intel-realsense-sdk-installation","title":"1.3 Intel RealSense SDK Installation","text":"<p>Intel RealSense is an RGB camera with channels designed for depth perception capabilities. The RealSense SDK 2.0 provides installation packages for Intel X86 / AMD64-based Debian distributions in <code>dpkg</code> format for Ubuntu 16/18/20 LTS. You'll be able to configure custom settings for any Intel RealSense cameras attached to the system to stream images to remote clients. </p> <p>Use Ubuntu to install Intel RealSense SDK 2.0 onto your computer system.</p>"},{"location":"setup/desktop-dep/#14-ffmpeg-installation","title":"1.4 FFmpeg Installation","text":"<p>FFmpeg is an open-source software project consisting of libraries and programs that handle video, audio, and other multimedia files and streams.</p> <p>To install FFmpeg, run the following commands on your Ubuntu terminal: <pre><code>sudo add-apt-repository ppa:jonathonf/ffmpeg-4\nsudo apt-get install ffmpeg libavcodec-dev libavdevice-dev libavfilter-dev libavformat-dev libavresample-dev libavutil-dev libpostproc-dev libswresample-dev libswscale-dev libgoogle-glog-dev\n</code></pre></p>"},{"location":"setup/desktop-dep/#15-python-requirements","title":"1.5 Python Requirements","text":"<p>The d.ASH SDK works with Python 3.7. To properly run the server, you will also need to install a python package installer, pip.</p>"},{"location":"setup/desktop-dep/#151-apt-get","title":"1.5.1 apt-get","text":"<p>Installed in Ubuntu and any Ubuntu-based Linux distribution, <code>apt-get</code> is tool for installing, upgrading, and cleaning packages. To set up the new environment, execute the following command on your Ubuntu terminal: <pre><code>sudo apt-get install -y python3.7-dev\n</code></pre></p>"},{"location":"setup/desktop-dep/#152-pip-installation","title":"1.5.2 Pip Installation","text":"<p>Pip is a package installer for Python. The d.ASH SDK and the third-party packages used by many of its programming examples use pip to install. To install pip, run the following command on your Ubuntu terminal:</p> <pre><code>sudo apt install python3.7 python3-pip python-pip\npython3.7 -m pip instal -- upgrade pip\n</code></pre>"},{"location":"setup/payload-reg/","title":"Payload Registration","text":"<p>Before running the d.ASH SDK, please make sure that all of your credentials have been set up correctly. This means using the right username and the right password. This section of the d.ASH SDK documentation provides details about setting up credentials for the d.ASH server, your robot, and d.ASH autonomy. Files in this section can be found in the folder <code>registration</code>:</p> <pre><code>dash-sdk/\n\u2514\u2500 registration\n    \u2514\u2500 set_spot_cred\n    \u2514\u2500 register_payload\n    \u2514\u2500 set_autonomy_cred\n</code></pre>"},{"location":"setup/payload-reg/#21-dash-server-credentials","title":"2.1 d.ASH Server Credentials","text":"<p>To set up the local credentials for the d.ASH server, you will need to run the file <code>set_spot_cred</code>. Run the following command replacing <code>&lt;USERNAME&gt;</code> with your chosen username and  <code>&lt;PASSWORD&gt;</code> with your chosen password.</p> <pre><code>./set_spot_cred -u &lt;USERNAME&gt; -p &lt;PASSWORD&gt;\n</code></pre> <p>For example, if your username is <code>user123</code> and your password is <code>pw123</code>, your command would look like this: <pre><code>./set_spot_cred -u user123 -p pw123\n</code></pre></p> <p>Username in Robot Configuration</p> <p>Please ensure that the username for the d.ASH server is the same as the one defined in the robot configuration file - <code>robot_config.json</code>. That is, you would replace <code>&lt;USERNAME&gt;</code> with your chosen username in <code>\"username\" : \"&lt;USERNAME&gt;\"</code>.</p>"},{"location":"setup/payload-reg/#22-robot-registration","title":"2.2 Robot Registration","text":"<p>To register the payload computer with d.ASH's backend system, you will need to run the file <code>register_payload</code>. However, you will first need to configure the <code>register_payload_config.json</code> found in the <code>\\dash-sdk\\configs</code> folder of the SDK. Set <code>&lt;ROBOT_NAME&gt;</code> to any name you like, and set <code>&lt;DC_USERNAME&gt;</code> to your dConstruct cloud admin username. </p> <p>For example, if your robot name is <code>robot1</code> and your cloud admin user name is <code>user123</code>, your <code>register_payload_config.json</code> would look like this:</p> <pre><code>{\n    RobotName: robot1,\n    RobotUserName: user123\n}\n</code></pre> <p>Now, run the following command to register your robot, replacing <code>&lt;PATH_TO_SDK&gt;</code> with your local path to the d.ASH SDK.</p> <pre><code>./register_payload -i &lt;PATH_TO_SDK&gt;/dash_sdk/configs/register_payload_config\n</code></pre>"},{"location":"setup/payload-reg/#23-dash-autonomy-credentials","title":"2.3 d.ASH Autonomy Credentials","text":"<p>To set up the local credentials for d.ASH autonomy, you will need to run the file  <code>set_autonomy_cred</code>. Run the following command replacing <code>&lt;USERNAME&gt;</code> with your cloud admin username. </p> <p><pre><code>cd ./set_auto_cred/build\n./set_autonomy_cred -u &lt;USERNAME&gt; \n</code></pre> For example, if your username is <code>user123</code>, your command would look like this: <pre><code>cd ./set_auto_cred/build\n./set_autonomy_cred -u user123\n</code></pre> You will then be prompted to enter a password, which will match your cloud admin password. Enter</p>"},{"location":"setup/vpn/","title":"Setting Up d.ASH VPN","text":"<p>When setting up VPN for d.ASH, two separate login credentials are required for two seperate VPN connections - one for the robot onboard computer and one for the remote client. This section of the d.ASH SDK documentation provides details about setting up the d.ASH VPN for both your robot and your remote client.</p>"},{"location":"setup/vpn/#31-setting-up-vpn-onboard-computer","title":"3.1 Setting Up VPN Onboard Computer","text":"<p>To start, you will need to install some packages to configure automatic VPN connection on Ubuntu 18.04 LTS by executing the following command:</p> <pre><code>sudo apt install network-manager-openvpn network-manager-openvpn-gnome openvpn openvpn-systemd-resolved -y\n</code></pre> <p>This will install an <code>openvpn</code> package, which creates a <code>/etc/openvpn/client/</code> directory into which you can place the OpenVPN client configuration file. You will need to configure the VPN configuration file - <code>client.ovpn</code> which can be found in your vpn folder <code>/dash_sdk/vpn</code>. </p> <p><pre><code>dash-sdk/\n\u2514\u2500 vpn/\n    \u2514\u2500 client.ovpn\n    \u2514\u2500 ca.crt\n    \u2514\u2500 &lt;USER&gt;.crt\n    \u2514\u2500 &lt;USER&gt;.key\n</code></pre> Note that <code>&lt;USER&gt;</code> in this instance is replaced by your dConstruct admin username. Now, you will need to copy <code>client.ovpn</code> and your user certifications - <code>ca.crt</code>, <code>&lt;USER&gt;.crt</code>, <code>&lt;USER&gt;.key</code> - into the new open vpn directory. In the <code>/dash_sdk</code> directory, execute the following commands:</p> <pre><code>python3.7 config_vpn.py\nsudo cp client.ovpn /etc/openvpn/client/client.conf \nsudo cp ca.crt &lt;USER&gt;.crt &lt;USER&gt;.key /etc/openvpn/client\n</code></pre> <p>For example, if your dConstruct admin username is <code>user123</code>, you would replacing <code>&lt;USER&gt;</code> with <code>user123</code>:</p> <pre><code>sudo cp client.ovpn /etc/openvpn/client/client.conf \nsudo cp ca.crt user123.crt user123.key /etc/openvpn/client\n</code></pre> <p>To check that your files have been copied and renamed correctly, <code>cd</code> into the <code>/etc/openvpn/client</code> directory and <code>ls</code> to see your list of files. You should have <code>client.conf</code> and your user certification files, namely <code>ca.crt &lt;USER&gt;.crt &lt;USER&gt;.key</code>:</p> <pre><code>etc/\n\u2514\u2500 openvpn/\n    \u2514\u2500 client/\n        \u2514\u2500 client.conf\n        \u2514\u2500 ca.crt\n        \u2514\u2500 &lt;USER&gt;.crt\n        \u2514\u2500 &lt;USER&gt;.key\n</code></pre> <p>Now, let's test that the VPN service was set up correctly by running the following command: <pre><code>sudo systemctl start openvpn-client@client.service\n</code></pre></p> <p>If there is no error print, proceed onto the next step. If you come across a failure, ensure that the path in <code>client.conf</code> matches the following format:</p> <pre><code>cat \\etc\\openvpn\\client\\ca.crt\ncert \\etc\\openvpn\\client\\&lt;USER&gt;.crt\nkey \\etc\\openvpn\\client\\&lt;USER&gt;.crt\n</code></pre> <p>Now, to check your VPN status, enter the following command:  <pre><code>sudo systemctl status openvpn-client@client.service\n</code></pre> If successful, you should be able to see the status <code>Initialization Sequence Completed</code>. Lastly, enable the VPN onboard your computer by executing the following command: </p> <pre><code>sudo systemctl enable openvpn-client@client.service\n</code></pre>"},{"location":"setup/vpn/#32-setting-up-vpn-remote-client","title":"3.2 Setting Up VPN Remote Client","text":"<p>Remember to use a separate login credential from the robot onboard computer credentials as at any point in time, there can only be one active user session.</p> <p>Firstly, download OpenVPN Connect. Once OpenVPN has been launched, click on the tab - 'Import from File' tab and drag-and-drop the <code>client.ovpn</code> file located in <code>\\dash-sdk\\vpn</code>. It is important to note that the <code>client.ovpn</code> file has to be in the same directory as there certification files, namely <code>ca.crt &lt;USER&gt;.crt &lt;USER&gt;.key</code>:</p> <pre><code>dash-sdk/\n\u2514\u2500 vpn/\n    \u2514\u2500 client.ovpn\n    \u2514\u2500 ca.crt\n    \u2514\u2500 &lt;USER&gt;.crt\n    \u2514\u2500 &lt;USER&gt;.key\n</code></pre>"}]}